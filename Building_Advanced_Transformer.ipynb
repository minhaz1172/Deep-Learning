{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqxHEVbvXzGeX2/vv0pe9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhaz1172/Deep-Learning/blob/main/Building_Advanced_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**implement and experiment with advanced Transformer models using Keras.**"
      ],
      "metadata": {
        "id": "fqVO6haVtbSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Liabraries"
      ],
      "metadata": {
        "id": "aYSaHYvGvQgS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vVqx2Mp8s8BF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf # Deep learning framework for building and training neural network models.\n",
        "import requests    # For making HTTP requests (might be used later to fetch online data).\n",
        "from sklearn.preprocessing import MinMaxScaler # Utility for scaling data to the range [0, 1].\n",
        "from tensorflow.keras.layers import Layer,Dense,LayerNormalization,Dropout\n",
        "# These Keras layers are used to build custom neural network components (like in a Transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  setup the environment to generate random synthetic stock price data data to use as dataset"
      ],
      "metadata": {
        "id": "ydkVUtUMtaVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42) # NumPy's random number generator.\n",
        "# To ensure reproducibility—every time you run the code, the same random numbers will be generated, so the results are consistent.\n",
        "data_length=2000 # Defines the number of data points.2000 means you're simulating stock prices for 2000 time steps.\n",
        "\n",
        "trend=np.linspace(100,200,data_length) # np.linspace(start, end, num) returns num equally spaced values from start to end. So, this line generates 2000 points starting at 100 and ending at 200, forming a straight upward trend.\n",
        " # np.random.normal(mean, std, size) generates random values from a normal (Gaussian) distribution.\n",
        "\n",
        "# mean=0: Noise is centered around zero, so it doesn’t add a bias.\n",
        "\n",
        "# std=2: Standard deviation of 2 adds moderate variation.\n",
        "\n",
        "# size=data_length: Generates 2000 noise values, one for each dat\n",
        "noise=np.random.normal(0,2,data_length)\n",
        "\n",
        "syntheic_data=trend+noise\n",
        "#Combines the trend with random noise to simulate a more realistic stock price behavior.\n",
        "# Result: A dataset (synthetic_data) that looks like a stock price gradually rising over time, but with small random ups and downs (just like real market data)."
      ],
      "metadata": {
        "id": "yhhT-WY7woj5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create a dataframe and save as stock_pricees.csv"
      ],
      "metadata": {
        "id": "TDf4Zegh0teo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.DataFrame(syntheic_data,columns=['Close']) #: Names the single column as \"Close\"\n",
        "data.to_csv('stock_prices.csv',index=False) # index=False: Ensures the DataFrame index (row numbers) is not included as a separate column in the CSV file.\n",
        "print(\"Synthetic stock_prices.csv created and loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kx4OAzP012F",
        "outputId": "04cfb9eb-2bf7-421f-f7d7-3583da130525"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic stock_prices.csv created and loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "ClynZECj1hBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('stock_prices.csv')\n",
        "data=data[['Close']].values # Select only the 'Close' column and convert it to a NumPy array for easier numerical processing."
      ],
      "metadata": {
        "id": "7Gq820H41mZT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize the data and fit the scalar to data"
      ],
      "metadata": {
        "id": "0Lbcyphk-DPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the MinMaxScaler to transform data values into the range between 0 and 1.\n",
        "scaler=MinMaxScaler(feature_range=(0,1))\n",
        "data=scaler.fit_transform(data) ## Fit the scaler to the data and transform it, which normalizes the stock price values.\n"
      ],
      "metadata": {
        "id": "iS5VJibJ-RgK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data for training"
      ],
      "metadata": {
        "id": "srwS_s29_bS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a function to prepare the data for time series prediction.\n",
        "# time_step: The number of past time steps to use as input features (X) to predict the next value (Y).\n",
        "\n",
        "def create_dataset(data, time_step=1):\n",
        "    \"\"\"\n",
        "    This function creates the dataset in a sliding window fashion.\n",
        "    For each index 'i', it takes 'time_step' consecutive values as input (X)\n",
        "    and the next immediate value as the corresponding label (Y).\n",
        "\n",
        "    Example:\n",
        "    Input: [10, 20, 30] → Output: 40\n",
        "    Input: [20, 30, 40] → Output: 50\n",
        "    Input: [30, 40, 50] → Output: 60\n",
        "\n",
        "    It transforms a 1D sequence into multiple input-output pairs that\n",
        "    a machine learning or deep learning model can learn from.\n",
        "    \"\"\"\n",
        "    X, Y = [], []  # Initialize empty lists to store input sequences and their corresponding outputs.\n",
        "\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        a = data[i:(i + time_step), 0]         # Extract a window of 'time_step' values from 'data' starting at index 'i'\n",
        "        X.append(a)                            # Append this extracted sequence to the input list (X)\n",
        "        Y.append(data[i + time_step, 0])       # Append the next value as the target (Y)\n",
        "\n",
        "    return np.array(X), np.array(Y)            # Convert the lists into NumPy arrays for model input\n",
        "\n",
        "\n",
        "# Define the length of the time sequence to consider when predicting the next stock price\n",
        "time_step = 100\n",
        "\n",
        "# Generate the training features (X) and labels (Y) using the 'create_dataset' function\n",
        "X, Y = create_dataset(data, time_step)  # X: shape (samples, 100); Y: next value for each input sequence\n",
        "\n",
        "# Reshape X to be [samples, time steps, features]\n",
        "X = X.reshape(\n",
        "    X.shape[0],    # Number of samples (windows)\n",
        "    X.shape[1],    # Time steps per sample\n",
        "    1              # One feature (e.g., closing price)\n",
        ")\n",
        "\n",
        "# Now X is 3D, as required by LSTM or similar deep learning models\n",
        "print(\"Shape of X:\",X.shape) ## Expected: (Number of samples, time_step, 1)\n",
        "print(\"Shape of Y:\",Y.shape) ## Expected: (Number of samples,)"
      ],
      "metadata": {
        "id": "AOWByQcW_e2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1d7b59-d03b-4031-ff3b-f96774b356f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (1899, 100, 1)\n",
            "Shape of Y: (1899,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiheadSelfAttention"
      ],
      "metadata": {
        "id": "yfzqFPABw_3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "class MultiHeadSelfAttention(Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()  # Initialize the parent Layer class\n",
        "\n",
        "        self.embed_dim = embed_dim                     # Total embedding dimension of the model (e.g., 512)\n",
        "        self.num_heads = num_heads                     # Number of attention heads (e.g., 8)\n",
        "        self.projection_dim = embed_dim // num_heads   # Dimensionality of each head (e.g., 64 if 512/8)\n",
        "\n",
        "        # 💡 Dense layers to generate queries, keys, and values from the input\n",
        "        self.query_dense = Dense(embed_dim)            # Linear layer to create the query matrix\n",
        "        self.key_dense = Dense(embed_dim)\n",
        "        self.value_dense = Dense(embed_dim)            # Linear layer to create the value matrix\n",
        "\n",
        "        # 💡 After attention, all heads are concatenated and projected back to original embed_dim\n",
        "        self.combine_heads = Dense(embed_dim)          # Linear layer to combine outputs from all heads\n",
        "\n",
        "    # The attention() method implements the core attention mechanism.\n",
        "    def attention(self, query, key, value):\n",
        "        # Compute the raw attention scores by taking the dot product of query and the transpose of key\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "        # Scale the scores by the square root of the key dimension (to stabilize gradients)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "\n",
        "        # Apply softmax to obtain attention weights (probabilities that sum to 1)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "\n",
        "        # Multiply the weights with the value vectors to get the final output\n",
        "        output = tf.matmul(weights, value)\n",
        "\n",
        "        return output, weights  # Return both the attention output and the weights\n",
        "\n",
        "    # Helper method to split the last dimension into multiple heads.\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Reshape the tensor to [batch_size, sequence_length, num_heads, projection_dim].\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        # Transpose to [batch_size, num_heads, sequence_length, projection_dim]\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Get the current batch size from the input.\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # Project the inputs into query, key, and value\n",
        "        query = self.query_dense(inputs)  # (batch_size, sequence_length, embed_dim)\n",
        "        key = self.key_dense(inputs)      # Same shape as query\n",
        "        value = self.value_dense(inputs)  # Same shape as query\n",
        "\n",
        "        # Split the query, key, and value into multiple heads\n",
        "        query = self.split_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # Compute attention: apply dot-product attention\n",
        "        attention, _ = self.attention(query, key, value)\n",
        "\n",
        "        # Transpose back to (batch_size, sequence_length, num_heads, projection_dim)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # Concatenate all heads back together\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "\n",
        "        # Final dense layer to combine all heads’ outputs\n",
        "        output = self.combine_heads(concat_attention)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "TEkQW3uzAzZq"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}